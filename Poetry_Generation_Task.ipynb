{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb7_Vc0kheYO",
        "outputId": "dfff7b41-1f64-4761-81fd-1d2c0ca7ceca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the uploaded ZIP file (It will be in /content)\n",
        "zip_path = 'dataset.zip'  # Replace with your uploaded file name\n",
        "extract_path = 'dataset'  # Directory to extract to\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uBJoKIPmjQp"
      },
      "source": [
        "Verify the Dataset Folder Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoWrOqDImkbc",
        "outputId": "5a8a7577-742a-4280-841d-56d59b137ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Poets Folder: ['.DS_Store', 'ahmad-faraz', 'akbar-allahabadi', 'allama-iqbal', 'altaf-hussain-hali', 'ameer-khusrau', 'bahadur-shah-zafar', 'dagh-dehlvi', 'fahmida-riaz', 'faiz-ahmad-faiz', 'firaq-gorakhpuri', 'gulzar', 'habib-jalib', 'jaan-nisar-akhtar', 'jaun-eliya', 'javed-akhtar', 'jigar-moradabadi', 'kaifi-azmi', 'meer-anees', 'meer-taqi-meer', 'mirza-ghalib', 'mohsin-naqvi', 'naji-shakir', 'naseer-turabi', 'nazm-tabatabai', 'nida-fazli', 'noon-meem-rashid', 'parveen-shakir', 'sahir-ludhianvi', 'wali-mohammad-wali', 'waseem-barelvi']\n"
          ]
        },
        {
          "ename": "NotADirectoryError",
          "evalue": "[WinError 267] The directory name is invalid: 'dataset/dataset\\\\.DS_Store'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m sample_poet_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, sample_poet)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# List the subfolders inside the poet's folder (should be 'en', 'ur', 'hi')\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubfolders for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_poet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_poet_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid: 'dataset/dataset\\\\.DS_Store'"
          ]
        }
      ],
      "source": [
        "# Update the dataset directory path\n",
        "dataset_dir = 'dataset/dataset'  # Adjusted path\n",
        "\n",
        "# List the poets' folders\n",
        "print(\"Poets Folder:\", os.listdir(dataset_dir))\n",
        "\n",
        "# Pick the first poet's folder\n",
        "sample_poet = os.listdir(dataset_dir)[0]\n",
        "sample_poet_path = os.path.join(dataset_dir, sample_poet)\n",
        "\n",
        "# List the subfolders inside the poet's folder (should be 'en', 'ur', 'hi')\n",
        "print(f\"Subfolders for {sample_poet}: {os.listdir(sample_poet_path)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3vDeNU2oq5h"
      },
      "source": [
        "Load & Read the Poetry Files from \"en\" Folders\n",
        "Now, we need to extract poetry only from the \"en\" folder of each poet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5wrkcBvorhU",
        "outputId": "392468f8-e89b-4202-d56f-72e52f441427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Poems Collected: 1314\n",
            "Sample Poem:\n",
            " \n",
            "aañkh se duur na ho dil se utar jā.egā \n",
            "vaqt kā kyā hai guzartā hai guzar jā.egā \n",
            "itnā mānūs na ho ḳhalvat-e-ġham se apnī \n",
            "tū kabhī ḳhud ko bhī dekhegā to Dar jā.egā \n",
            "Dūbte Dūbte kashtī ko uchhālā de duuñ \n",
            "maiñ nahīñ koī to sāhil pe utar jā.egā \n",
            "zindagī terī atā hai to ye jaane vaalā \n",
            "terī baḳhshish tirī dahlīz pe dhar jā.egā \n",
            "zabt lāzim hai magar dukh hai qayāmat kā 'farāz' \n",
            "zālim ab ke bhī na ro.egā to mar jā.egā \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to the dataset containing poet folders\n",
        "dataset_dir = 'dataset/dataset'\n",
        "\n",
        "# List to store all poetry texts\n",
        "poetry_texts = []\n",
        "\n",
        "# Iterate through each poet's folder\n",
        "for poet in os.listdir(dataset_dir):\n",
        "    poet_path = os.path.join(dataset_dir, poet)\n",
        "\n",
        "    # Check if it's a directory (ignore files like .DS_Store)\n",
        "    if os.path.isdir(poet_path):\n",
        "        en_path = os.path.join(poet_path, \"en\")  # Path to \"en\" poetry folder\n",
        "\n",
        "        # Check if \"en\" folder exists\n",
        "        if os.path.exists(en_path):\n",
        "            for file in os.listdir(en_path):\n",
        "                file_path = os.path.join(en_path, file)\n",
        "\n",
        "                # Read the poetry file and append to list\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    poetry_texts.append(f.read())\n",
        "\n",
        "# Print sample poetry to verify\n",
        "print(f\"Total Poems Collected: {len(poetry_texts)}\")\n",
        "print(\"Sample Poem:\\n\", poetry_texts[0][:500])  # Print first 500 characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygkcb1IppS1a"
      },
      "source": [
        "Preprocess the Text Data\n",
        "Before feeding data into the model, we need to:\n",
        "\n",
        "Lowercase the text (optional)\n",
        "Remove unwanted characters & extra spaces\n",
        "Tokenize the text into sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzPO9DGlo50T",
        "outputId": "a1b58748-e468-473c-b82e-5d03c74d0f2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned Sample:\n",
            " shahīd-e-ishq hue qais nāmvar kī tarah jahāñ meñ aib bhī ham ne kiye hunar kī tarah kuchh aaj shaam se chehra hai faq sahar kī tarah Dhalā hī jaatā huuñ furqat meñ dopahar kī tarah siyāh-baḳhtoñ ko yuuñ baaġh se nikāl ai charḳh ki chaar phuul to dāman meñ hoñ sipar kī tarah tamām ḳhalq hai ḳhvāhān-e-ābrū ai rab chhupā mujhe sadaf-e-qabr meñ guhar kī tarah tujhī ko dekhūñgā jab tak haiñ barqarār āñkheñ mirī nazar na phiregī tirī nazar kī tarah hamārī qabr pe kyā ehtiyāj-e-ambar-o-ūd sulag rahā ha\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters except basic punctuations\n",
        "    # text = re.sub(r'[^a-zA-Z0-9.,!?\\'\\s]', '', text)\n",
        "\n",
        "    # Convert multiple spaces into a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to all poems\n",
        "cleaned_poetry = [preprocess_text(poem) for poem in poetry_texts]\n",
        "\n",
        "# Print a cleaned sample\n",
        "print(\"Cleaned Sample:\\n\", cleaned_poetry[0][:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Z3AVZnqBzc"
      },
      "source": [
        "Convert Poetry into Sequences\n",
        "Since LSTM models require numerical input, we need to:\n",
        "\n",
        "Tokenize words into unique integers\n",
        "Create input-output sequences (X: previous words, Y: next word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI-Op1nuqCy9",
        "outputId": "7cc0a932-ba43-4d66-80c3-9309519f858f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchtext==0.15.2\n",
            "  Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.3.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.4)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m332.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m198.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m210.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m216.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m251.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m201.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m241.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m248.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m180.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m224.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m249.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m287.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m213.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m246.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m234.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1 torchtext==0.15.2 --no-cache-dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_in_BdBHppwR",
        "outputId": "87887ae6-44bd-4c29-d2c0-bb6a2e16fca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 17344\n",
            "Encoded Sample Poem: [14417, 79, 950, 12695, 6, 93, 118, 4, 1082, 10]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Tokenize poetry into words\n",
        "def tokenize_poetry(poetry_list):\n",
        "    return [poem.split() for poem in poetry_list]\n",
        "\n",
        "# Build vocabulary\n",
        "tokenized_poems = tokenize_poetry(cleaned_poetry)\n",
        "vocab = build_vocab_from_iterator(tokenized_poems, specials=[\"<PAD>\", \"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "# Convert words to indices\n",
        "def encode_poetry(poem):\n",
        "    return [vocab[word] for word in poem]\n",
        "\n",
        "encoded_poems = [encode_poetry(poem) for poem in tokenized_poems]\n",
        "\n",
        "# Print vocabulary size\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")\n",
        "print(\"Encoded Sample Poem:\", encoded_poems[0][:10])  # Show first 10 encoded words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpIzaC3nsiRl"
      },
      "source": [
        "Prepare Data for Training\n",
        "We need to:\n",
        "\n",
        "Create sequences of words (e.g., first 5 words → predict next word)\n",
        "Pad sequences for uniform input size\n",
        "Split data into training & validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OunMnNxAsi9B",
        "outputId": "e736574a-ce1e-4047-e566-7f5371594046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Input: tensor([ 14, 271,  15,   2,  88])\n",
            "Sample Target: tensor(5141)\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, poems, seq_length=5):\n",
        "        self.seq_length = seq_length\n",
        "        self.data = []\n",
        "\n",
        "        # Create sequences\n",
        "        for poem in poems:\n",
        "            if len(poem) > seq_length:\n",
        "                for i in range(len(poem) - seq_length):\n",
        "                    seq = poem[i:i+seq_length]\n",
        "                    target = poem[i+seq_length]\n",
        "                    self.data.append((seq, target))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence, target = self.data[idx]\n",
        "        return torch.tensor(sequence), torch.tensor(target)\n",
        "\n",
        "# Create dataset\n",
        "seq_length = 5\n",
        "dataset = PoetryDataset(encoded_poems, seq_length)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Print sample batch\n",
        "for x, y in dataloader:\n",
        "    print(\"Sample Input:\", x[0])\n",
        "    print(\"Sample Target:\", y[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVnYSbXFsvrX"
      },
      "source": [
        "Build an LSTM Model in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgxDVxyYswTz",
        "outputId": "956b86c3-af71-4100-9f49-829ef850ec4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Initialized\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PoetryLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2):\n",
        "        super(PoetryLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "        # Layer Normalization (Added)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Apply Layer Normalization\n",
        "        lstm_out = self.layer_norm(lstm_out)\n",
        "\n",
        "        out = self.fc(lstm_out[:, -1])  # Use last LSTM output\n",
        "        return out\n",
        "\n",
        "# Model\n",
        "model = PoetryLSTM(vocab_size=len(vocab)).to(\"cuda\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Learning Rate Scheduler (Reduce LR after 25 epochs)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
        "\n",
        "print(\"Model Initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKsms8lPs4P2"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEuYeXzFs4w2",
        "outputId": "40791bd3-d96d-4a52-a3d2-7207649a21bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/51], Loss: 6.9408, LR: 0.000500\n",
            "Epoch [2/51], Loss: 6.0723, LR: 0.000500\n",
            "Epoch [3/51], Loss: 5.3268, LR: 0.000500\n",
            "Epoch [4/51], Loss: 4.5049, LR: 0.000500\n",
            "Epoch [5/51], Loss: 3.6548, LR: 0.000500\n",
            "Epoch [6/51], Loss: 2.9274, LR: 0.000500\n",
            "Epoch [7/51], Loss: 2.3332, LR: 0.000500\n",
            "Epoch [8/51], Loss: 1.8382, LR: 0.000500\n",
            "Epoch [9/51], Loss: 1.4150, LR: 0.000500\n",
            "Epoch [10/51], Loss: 1.0740, LR: 0.000500\n",
            "Epoch [11/51], Loss: 0.8096, LR: 0.000500\n",
            "Epoch [12/51], Loss: 0.6206, LR: 0.000500\n",
            "Epoch [13/51], Loss: 0.4935, LR: 0.000500\n",
            "Epoch [14/51], Loss: 0.4195, LR: 0.000500\n",
            "Epoch [15/51], Loss: 0.3781, LR: 0.000500\n",
            "Epoch [16/51], Loss: 0.3448, LR: 0.000500\n",
            "Epoch [17/51], Loss: 0.3219, LR: 0.000500\n",
            "Epoch [18/51], Loss: 0.3011, LR: 0.000500\n",
            "Epoch [19/51], Loss: 0.2874, LR: 0.000500\n",
            "Epoch [20/51], Loss: 0.2840, LR: 0.000500\n",
            "Epoch [21/51], Loss: 0.2617, LR: 0.000500\n",
            "Epoch [22/51], Loss: 0.2540, LR: 0.000500\n",
            "Epoch [23/51], Loss: 0.2502, LR: 0.000500\n",
            "Epoch [24/51], Loss: 0.2353, LR: 0.000500\n",
            "Epoch [25/51], Loss: 0.2351, LR: 0.000050\n",
            "Epoch [26/51], Loss: 0.0929, LR: 0.000050\n",
            "Epoch [27/51], Loss: 0.0245, LR: 0.000050\n",
            "Epoch [28/51], Loss: 0.0202, LR: 0.000050\n",
            "Epoch [29/51], Loss: 0.0183, LR: 0.000050\n",
            "Epoch [30/51], Loss: 0.0171, LR: 0.000050\n",
            "Epoch [31/51], Loss: 0.0167, LR: 0.000050\n",
            "Epoch [32/51], Loss: 0.0151, LR: 0.000050\n",
            "Epoch [33/51], Loss: 0.0151, LR: 0.000050\n",
            "Epoch [34/51], Loss: 0.0145, LR: 0.000050\n",
            "Epoch [35/51], Loss: 0.0141, LR: 0.000050\n",
            "Epoch [36/51], Loss: 0.0138, LR: 0.000050\n",
            "Epoch [37/51], Loss: 0.0137, LR: 0.000050\n",
            "Epoch [38/51], Loss: 0.0133, LR: 0.000050\n",
            "Epoch [39/51], Loss: 0.0131, LR: 0.000050\n",
            "Epoch [40/51], Loss: 0.0127, LR: 0.000050\n",
            "Epoch [41/51], Loss: 0.0129, LR: 0.000050\n",
            "Epoch [42/51], Loss: 0.0122, LR: 0.000050\n",
            "Epoch [43/51], Loss: 0.0124, LR: 0.000050\n",
            "Epoch [44/51], Loss: 0.0120, LR: 0.000050\n",
            "Epoch [45/51], Loss: 0.0120, LR: 0.000050\n",
            "Epoch [46/51], Loss: 0.0119, LR: 0.000050\n",
            "Epoch [47/51], Loss: 0.0117, LR: 0.000050\n",
            "Epoch [48/51], Loss: 0.0117, LR: 0.000050\n",
            "Epoch [49/51], Loss: 0.0114, LR: 0.000050\n",
            "Epoch [50/51], Loss: 0.0113, LR: 0.000005\n",
            "Epoch [51/51], Loss: 0.0074, LR: 0.000005\n",
            "Training Completed!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 51\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        x_batch, y_batch = x_batch.to(\"cuda\"), y_batch.to(\"cuda\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x_batch)\n",
        "\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.detach().item()  # Detach to save memory\n",
        "\n",
        "    # Reduce learning rate after step_size epochs\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(\"Training Completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvn3xg7Rs647",
        "outputId": "d831e93a-86c4-414f-c111-b7f42416d69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Poetry: ajab junūn-e-masāfat meñ ghar se niklā thā ḳhabar nahīñ hai ki sūraj kidhar se niklā thā ye kaun phir se unhīñ rāstoñ meñ chhoḌ gayā abhī abhī to azāb-e-safar se niklā thā ye tiir dil meñ magar be-sabab nahīñ utrā koī to harf lab-e-chāragar se\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_poetry(seed_text, model, vocab, max_words=40):\n",
        "    model.eval()\n",
        "    words = seed_text.split()\n",
        "\n",
        "    for _ in range(max_words):\n",
        "        encoded = torch.tensor([vocab[word] for word in words[-5:]]).unsqueeze(0).to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            output = model(encoded)\n",
        "            next_word = vocab.lookup_token(output.argmax().item())\n",
        "            words.append(next_word)\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Example Usage\n",
        "seed = \"ajab junūn-e-masāfat meñ ghar se\"\n",
        "print(\"Generated Poetry:\", generate_poetry(seed, model, vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "hGJDYHowb6br",
        "outputId": "1d4674ce-d308-4f7d-a52d-30384baf4324"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/data/poetry_lstm_model.pth'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'poetry_lstm_model.pth' -> '/mnt/data/poetry_lstm_model.pth'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-7c6d17dfcea9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Move the model to a location that can be downloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"poetry_lstm_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/mnt/data/poetry_lstm_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/data/poetry_lstm_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/poetry_lstm_model.pth'"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"poetry_lstm_model.pth\")\n",
        "import shutil\n",
        "\n",
        "# Move the model to a location that can be downloaded\n",
        "shutil.move(\"poetry_lstm_model.pth\", \"/mnt/data/poetry_lstm_model.pth\")\n",
        "from google.colab import files\n",
        "files.download(\"/mnt/data/poetry_lstm_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o7nicltnNe8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
